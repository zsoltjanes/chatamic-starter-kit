sections:
  main:
    display: Main
    fields:
      -
        handle: model
        field:
          input_type: text
          antlers: false
          default: text-davinci-003
          display: model
          type: text
          icon: text
          instructions: 'ID of the model to use.'
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
          validate:
            - required
      -
        handle: max_tokens
        field:
          default: '16'
          display: max_tokens
          type: integer
          icon: integer
          instructions: "The maximum number of tokens to generate in the completion.  The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096)."
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
      -
        handle: temperature
        field:
          default: '1'
          display: temperature
          type: float
          icon: float
          instructions: 'What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.'
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
      -
        handle: top_p
        field:
          default: '1'
          display: top_p
          type: float
          icon: float
          instructions: 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.'
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
      -
        handle: frequency_penalty
        field:
          default: '0'
          display: frequency_penalty
          type: float
          icon: float
          instructions: "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
      -
        handle: presence_penalty
        field:
          default: '0'
          display: presence_penalty
          type: float
          icon: float
          instructions: "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
          listable: hidden
          instructions_position: above
          visibility: visible
          always_save: false
